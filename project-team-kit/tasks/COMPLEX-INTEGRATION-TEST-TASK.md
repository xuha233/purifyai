# 复杂集成测试任务 - PurifyAI v0.7.0 RC1

**任务来源：** Project Team Kit 任务清单  
**Assigned To:** Kimi Code  
**Priority:** High  
**Estimated Time:** 30-60 分钟  
**Start Time:** 2026-02-24 11:30

---

## 1. 测试目标

本测试旨在验证 PurifyAI v0.7.0 RC1 的以下核心功能在真实环境下的表现：

- [ ] **功能 1:** 完整的 4 阶段清理流程（Scan → Review → Cleanup → Report）
- [ ] **功能 2:** AI 成本控制的实际运行和降级机制
- [ ] **功能 3:** 错误处理和自动恢复机制
- [ ] **功能 4:** UI 性能优化效果（大数据量场景）
- [ ] **功能 5:** 智能体系统的稳定性

---

## 2. 测试环境

**项目目录：** `G:\docker\diskclean\`

**Python 版本：** 3.14.3

**依赖项已安装：** pytest, PyQt5, anthropic, 其他依赖

---

## 3. 测试步骤

### 测试 1: 完整清理流程验证

**目标：** 验证从启动应用到完成清理的完整流程

**步骤：**

1. 启动应用：
   ```bash
   cd G:\docker\diskclean
   python src/main.py
   ```

2. 在应用界面中：
   - 选择扫描目标（例如：`C:\Users\YourName\AppData\Local\Temp`）
   - 启动扫描
   - 等待扫描完成

3. 验证扫描结果：
   - 检查是否显示扫描的项目列表
   - 检查项目的详细信息（路径、大小、风险等级）

4. 进入 Review 阶段：
   - 使用智能体分析可疑项目
   - 观察成本控制组件显示的费用信息

5. 进入 Cleanup 阶段：
   - 选择要删除的项目
   - 执行清理

6. 查看报告：
   - 检查清理报告是否正确显示

**验证标准：**
- ✅ 应用能够正常启动
- ✅ 扫描能够完成并显示结果
- ✅ AI 分析能够运行（或降级到规则引擎）
- ✅ 清理能够成功执行
- ✅ 报告能够正确显示

---

### 测试 2: AI 成本控制验证

**目标：** 验证成本控制机制的实际运行

**步骤：**

1. 检查配置文件（`config.json`）中的成本控制设置：
   ```json
   {
     "ai": {
       "cost_control": {
         "mode": "budget",
         "max_calls_per_scan": 1000,
         "max_budget_per_scan": 2.0,
         "fallback_to_rules": true
       }
     }
   }
   ```

2. 运行一个扫描任务，观察：
   - 成本组件的实时更新
   - 如果达到限制，是否自动降级到规则引擎

3. 检查日志文件，确认：
   - API 调用次数的记录
   - 成本的记录
   - 降级事件的记录

**验证标准：**
- ✅ 成本控制组件正常显示
- ✅ 达到限制时自动降级
- ✅ 日志记录完整

---

### 测试 3: 错误处理和自动恢复

**目标：** 验证错误处理机制的有效性

**步骤：**

1. 模拟一个错误场景：
   - 方式 1：暂时禁用网络连接，观察 AI 调用失败时是否降级
   - 方式 2：扫描一个不存在的路径，观察错误提示是否清晰

2. 观察用户界面的错误提示：
   - 检查错误信息是否清晰易懂
   - 检查是否有恢复建议

3. 检查日志文件（`logs/error.log` 或类似）：
   ```bash
   # 查找错误日志
   grep -r "ERROR" logs/
   ```

**验证标准：**
- ✅ 错误发生时能够捕获并记录
- ✅ 用户界面显示清晰的错误提示
- ✅ 自动恢复机制生效

---

### 测试 4: UI 性能优化验证

**目标：** 验证 UI 性能优化后的实际表现

**步骤：**

1. 启动应用并执行一个大扫描任务（例如扫描 `C:\Users\YourName`）

2. 观察以下场景的响应时间：
   - 页面切换（例如从 Dashboard → Agent Hub）是否流畅
   - 列表滚动（如果有大量项目）是否卡顿
   - AI 响应期间，UI 是否保持响应

3. 如果有性能监控工具，可以查看：
   - CPU 使用率
   - 内存使用率
   - 帧率

**验证标准：**
- ✅ 页面切换流畅（无明显延迟）
- ✅ 列表滚动流畅（无卡顿）
- ✅ AI 响应期间 UI 保持响应

---

### 测试 5: 智能体系统稳定性验证

**目标：** 验证智能体系统在长时间运行中的稳定性

**步骤：**

1. 运行多个连续的清理任务（例如 3-5 轮）

2. 观察每个任务是否：
   - 正常完成
   - 没有内存泄漏
   - 没有累积的错误

3. 检查状态：
   - 任务完成后，系统是否恢复到初始状态
   - 没有残留的临时文件或资源

**验证标准：**
- ✅ 多个任务连续运行正常
- ✅ 没有内存泄漏
- ✅ 任务完成后系统清理干净

---

## 4. 测试报告要求

**请在测试完成后，提供以下信息：**

### 测试结果汇总

| 测试项 | 通过 | 失败 | 备注 |
|--------|------|------|------|
| 完整清理流程 | ☐ | ☐ |  |
| AI 成本控制 | ☐ | ☐ |  |
| 错误处理和恢复 | ☐ | ☐ |  |
| UI 性能优化 | ☐ | ☐ |  |
| 智能体系统稳定性 | ☐ | ☐ |  |

### 发现的问题

如果有任何问题或 bug，请详细描述：

**问题描述：**
[填写]

**重现步骤：**
1.
2.
3.

**预期行为：**
[填写]

**实际行为：**
[填写]

**严重程度：** [Critical / High / Medium / Low]

### 性能指标（如果可测量）

| 指标 | 测量值 | 备注 |
|------|--------|------|
| 页面切换延迟 | ___ ms |  |
| 列表渲染时间（1000 项） | ___ ms |  |
| 扫描时间（1000 个文件） | ___ 秒 |  |
| 内存使用（峰值） | ___ MB |  |

### 建议和改进

如果有任何建议或发现的改进点，请记录：

[填写]

---

## 5. 注意事项

1. **安全第一：** 在测试清理功能时，请选择测试目录或临时目录，**不要清理系统关键文件**。

2. **日志记录：** 测试过程中建议保留完整的日志文件，以便问题排查。

3. **环境一致性：** 确保测试环境与生产环境尽可能一致。

4. **截图：** 如果发现 UI 问题，建议截图。

5. **耐心：** 完整的集成测试可能需要 30-60 分钟，请充分测试每个场景。

---

## 6. 如果测试失败

如果某个测试失败，请：

1. 记录详细的错误信息
2. 保存日志文件
3. 检查是否是环境问题（例如网络、权限）
4. 如果是代码问题，可以在 `TASK-BACKLOG.md` 中添加新的 Bug 记录

---

## 7. 提交报告

**测试完成后，报告应该保存在：**

```
G:\docker\diskclean\test-reports\complex-integration-test-report.md
```

**如果 `test-reports` 目录不存在，请先创建：**

```bash
mkdir G:\docker\diskclean\test-reports
```

---

**祝你测试顺利！如有问题，随时联系 OpenClaw。**

---

**版本：** v0.7.0 RC1  
**创建时间：** 2026-02-24 11:30  
**创建者：** 小午（OpenClaw）
